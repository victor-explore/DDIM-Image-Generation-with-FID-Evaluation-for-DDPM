{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'  # Prevent OpenMP initialization error\n",
    "import torch  # Import PyTorch library\n",
    "import torch.nn as nn  # Import neural network module\n",
    "import torch.optim as optim  # Import optimization module\n",
    "import math  # Import math module for log calculations\n",
    "from torchvision import datasets, transforms  # Import datasets and transforms\n",
    "from torchvision.utils import save_image, make_grid  # Import utility to save images\n",
    "import torchvision  # Import torchvision library\n",
    "import matplotlib.pyplot as plt  # Import plotting library\n",
    "import os  # Import os module for file operations\n",
    "import numpy as np  # Import numpy library\n",
    "from torch.utils.data import Dataset  # Add this import at the top\n",
    "from PIL import Image  # Import PIL Image module for image handling\n",
    "import torch.nn.functional as F  # Import PyTorch's functional API for loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set device\n",
    "print(f\"Using device: {device}\")  # Print the device being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the DDPM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net architecture for noise prediction in diffusion models with built-in residual connections, optimized for 128x128 RGB images\"\"\"\n",
    "    def __init__(self, in_channels=3, time_dim=256):  # Modified for RGB input (3 channels)\n",
    "        super().__init__()\n",
    "\n",
    "        # Pooling and activation layers used throughout the network\n",
    "        self.pool = nn.MaxPool2d(2)  # Max pooling for downsampling\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)  # Bilinear upsampling\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "\n",
    "        # Encoder Block 1 - Input level (128x128)\n",
    "        self.enc1_conv1 = nn.Conv2d(in_channels, 64, 3, padding=1)  # First conv: (N,3,128,128) -> (N,64,128,128)\n",
    "        self.enc1_bn1 = nn.BatchNorm2d(64)  # Normalizes each of the 64 channels independently\n",
    "        self.enc1_conv2 = nn.Conv2d(64, 64, 3, padding=1)  # Second conv: (N,64,128,128) -> (N,64,128,128)\n",
    "        self.enc1_bn2 = nn.BatchNorm2d(64)  # Batch norm after second conv\n",
    "\n",
    "        # Encoder Block 2 - After first pooling (64x64)\n",
    "        self.enc2_conv1 = nn.Conv2d(64, 128, 3, padding=1)  # First conv: (N,64,64,64) -> (N,128,64,64)\n",
    "        self.enc2_bn1 = nn.BatchNorm2d(128)  # Batch norm after first conv\n",
    "        self.enc2_conv2 = nn.Conv2d(128, 128, 3, padding=1)  # Second conv: (N,128,64,64) -> (N,128,64,64)\n",
    "        self.enc2_bn2 = nn.BatchNorm2d(128)  # Batch norm after second conv\n",
    "\n",
    "        # Encoder Block 3 - After second pooling (32x32)\n",
    "        self.enc3_conv1 = nn.Conv2d(128, 256, 3, padding=1)  # First conv: (N,128,32,32) -> (N,256,32,32)\n",
    "        self.enc3_bn1 = nn.BatchNorm2d(256)  # Batch norm after first conv\n",
    "        self.enc3_conv2 = nn.Conv2d(256, 256, 3, padding=1)  # Second conv: (N,256,32,32) -> (N,256,32,32)\n",
    "        self.enc3_bn2 = nn.BatchNorm2d(256)  # Batch norm after second conv\n",
    "\n",
    "        # Decoder Block 3 - First upsampling (32x32 -> 64x64)\n",
    "        self.dec3_conv1 = nn.Conv2d(384, 128, 3, padding=1)  # First conv: (N,384,64,64) -> (N,128,64,64)\n",
    "        self.dec3_bn1 = nn.BatchNorm2d(128)  # Batch norm after first conv\n",
    "        self.dec3_conv2 = nn.Conv2d(128, 128, 3, padding=1)  # Second conv: (N,128,64,64) -> (N,128,64,64)\n",
    "        self.dec3_bn2 = nn.BatchNorm2d(128)  # Batch norm after second conv\n",
    "\n",
    "        # Decoder Block 2 - Second upsampling (64x64 -> 128x128)\n",
    "        self.dec2_conv1 = nn.Conv2d(192, 64, 3, padding=1)  # First conv: (N,192,128,128) -> (N,64,128,128)\n",
    "        self.dec2_bn1 = nn.BatchNorm2d(64)  # Batch norm after first conv\n",
    "        self.dec2_conv2 = nn.Conv2d(64, 64, 3, padding=1)  # Second conv: (N,64,128,128) -> (N,64,128,128)\n",
    "        self.dec2_bn2 = nn.BatchNorm2d(64)  # Batch norm after second conv\n",
    "\n",
    "        # Final output layer\n",
    "        self.final_conv = nn.Conv2d(64, in_channels, kernel_size=1)  # Final conv: (N,64,128,128) -> (N,3,128,128)\n",
    "\n",
    "        # Time embedding dimension and projection\n",
    "        self.time_dim = time_dim  # Time embedding dimension\n",
    "\n",
    "        # Define MLPs as model parameters\n",
    "        self.time_enc1 = nn.Sequential(nn.Linear(time_dim, 64), nn.SiLU(), nn.Linear(64, 64))  # Time embedding MLP for encoder block 1\n",
    "        self.time_enc2 = nn.Sequential(nn.Linear(time_dim, 128), nn.SiLU(), nn.Linear(128, 128))  # Time embedding MLP for encoder block 2\n",
    "        self.time_enc3 = nn.Sequential(nn.Linear(time_dim, 256), nn.SiLU(), nn.Linear(256, 256))  # Time embedding MLP for encoder block 3\n",
    "        self.time_dec3 = nn.Sequential(nn.Linear(time_dim, 128), nn.SiLU(), nn.Linear(128, 128))  # Time embedding MLP for decoder block 3\n",
    "        self.time_dec2 = nn.Sequential(nn.Linear(time_dim, 64), nn.SiLU(), nn.Linear(64, 64))  # Time embedding MLP for decoder block 2\n",
    "\n",
    "    def get_time_embedding(self, t):\n",
    "        \"\"\"Generate sinusoidal time embedding and project through MLPs for each block\n",
    "\n",
    "        Args:\n",
    "            t: Time tensor of shape (batch_size, 1)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing time embeddings for each block\n",
    "        \"\"\"\n",
    "        half_dim = self.time_dim // 2  # Calculate half dimension for sin/cos embeddings\n",
    "        embeddings = torch.arange(half_dim, device=t.device).float()  # Create position indices\n",
    "        embeddings = torch.exp(-math.log(10000) * embeddings / half_dim)  # Calculate frequency bands\n",
    "        embeddings = t * embeddings.unsqueeze(0)  # Shape: (batch_size, half_dim)\n",
    "        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)  # Shape: (batch_size, time_dim)\n",
    "\n",
    "        # Use the class MLPs instead of creating new ones\n",
    "        t_emb_enc1 = self.time_enc1(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "        t_emb_enc2 = self.time_enc2(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "        t_emb_enc3 = self.time_enc3(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "        t_emb_dec3 = self.time_dec3(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "        t_emb_dec2 = self.time_dec2(embeddings).unsqueeze(-1).unsqueeze(-1)  # Use class MLP\n",
    "\n",
    "        return {\n",
    "            'enc1': t_emb_enc1,  # Time embedding for encoder block 1\n",
    "            'enc2': t_emb_enc2,  # Time embedding for encoder block 2\n",
    "            'enc3': t_emb_enc3,  # Time embedding for encoder block 3\n",
    "            'dec3': t_emb_dec3,  # Time embedding for decoder block 3\n",
    "            'dec2': t_emb_dec2   # Time embedding for decoder block 2\n",
    "        }\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"Forward pass through U-Net optimized for 128x128 RGB input with time embeddings at each block\"\"\"\n",
    "        # Time embedding\n",
    "        t = t.unsqueeze(-1).float()  # Ensure time is in correct shape\n",
    "        t_embs = self.get_time_embedding(t)  # Get time embeddings for each block\n",
    "\n",
    "        # Encoder pathway with skip connections and time embeddings\n",
    "        # Encoder Block 1 (128x128)\n",
    "        e1 = self.relu(self.enc1_bn1(self.enc1_conv1(x)))  # First conv layer\n",
    "        e1 = self.relu(self.enc1_bn2(self.enc1_conv2(e1)))  # Second conv layer with ReLU\n",
    "        e1 = e1 + t_embs['enc1']  # Add time embedding to encoder block 1\n",
    "\n",
    "        # Encoder Block 2 (64x64)\n",
    "        e2 = self.relu(self.enc2_bn1(self.enc2_conv1(self.pool(e1))))  # First conv layer\n",
    "        e2 = self.relu(self.enc2_bn2(self.enc2_conv2(e2)))  # Second conv layer with ReLU\n",
    "        e2 = e2 + t_embs['enc2']  # Add time embedding to encoder block 2\n",
    "\n",
    "        # Encoder Block 3 (32x32)\n",
    "        e3 = self.relu(self.enc3_bn1(self.enc3_conv1(self.pool(e2))))  # First conv layer\n",
    "        e3 = self.relu(self.enc3_bn2(self.enc3_conv2(e3)))  # Second conv layer with ReLU\n",
    "        e3 = e3 + t_embs['enc3']  # Add time embedding to encoder block 3\n",
    "\n",
    "        # Decoder pathway using skip connections\n",
    "        # Decoder Block 3 (32x32 -> 64x64)\n",
    "        d3 = torch.cat([self.upsample(e3), e2], dim=1)  # Concatenate along channel dimension\n",
    "        d3 = self.relu(self.dec3_bn1(self.dec3_conv1(d3)))  # First conv block\n",
    "        d3 = self.dec3_bn2(self.dec3_conv2(d3))  # Second conv block\n",
    "        d3 = d3 + t_embs['dec3']  # Add time embedding to decoder block 3\n",
    "\n",
    "        # Decoder Block 2 (64x64 -> 128x128)\n",
    "        d2 = torch.cat([self.upsample(d3), e1], dim=1)  # Concatenate along channel dimension\n",
    "        d2 = self.relu(self.dec2_bn1(self.dec2_conv1(d2)))  # First conv block\n",
    "        d2 = self.dec2_bn2(self.dec2_conv2(d2))  # Second conv block\n",
    "        d2 = d2 + t_embs['dec2']  # Add time embedding to decoder block 2\n",
    "\n",
    "        return self.final_conv(d2)  # Return final output (N,3,128,128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the trained model and generate samples\n",
    "model_path = r\"D:\\Users\\VICTOR\\Desktop\\ADRL\\Assignment 3\\unet_model_epoch_14.pth\"\n",
    "loaded_model = UNet().to('cuda')\n",
    "\n",
    "# Load the checkpoint dictionary with weights_only=True for security\n",
    "checkpoint = torch.load(model_path, weights_only=True)\n",
    "\n",
    "# Extract the model state dict from the checkpoint\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Set model to evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "loaded_model = loaded_model.to(device)\n",
    "\n",
    "print(f\"Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDIM Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference process in diffusion models can be done in two ways:\n",
    "\n",
    "DDPM (Denoising Diffusion Probabilistic Models):\n",
    "1. Sample from the prior: Start by sampling $x_T \\sim \\mathcal{N}(0, I)$, which is the prior distribution.\n",
    "\n",
    "2. Reverse the diffusion process: Sequentially get $x_{t-1}$ from $x_t$ for $t = T, T-1, \\ldots, 1$ using\n",
    "   \n",
    "    $$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon(x_t, t)) + \\sigma_t z$$\n",
    "\n",
    "    where $z \\sim \\mathcal{N}(0, I)$ and $\\sigma_t^2 = \\beta_t = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t}(1-\\alpha_t)$\n",
    "\n",
    "3. Obtain the final sample: The final sample $x_0$ is obtained after completing the reverse process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "DDIM (Denoising Diffusion Implicit Models):\n",
    "1. Sample from the prior: Start by sampling $x_T \\sim \\mathcal{N}(0, I)$, which is the prior distribution.\n",
    "\n",
    "2. Reverse the diffusion process: Sequentially get $x_{t-1}$ from $x_t$ for selected timesteps using\n",
    "    \n",
    "    $$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\left(\\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t}\\epsilon_\\theta(x_t,t)}{\\sqrt{\\bar{\\alpha}_t}}\\right) + \\sqrt{1-\\bar{\\alpha}_{t-1}}\\epsilon_\\theta(x_t,t)$$\n",
    "\n",
    "    where $\\epsilon_\\theta$ is the learned noise predictor. Unlike DDPM, DDIM is deterministic and does not add\n",
    "    random noise at each step, allowing for fewer sampling steps while maintaining quality.\n",
    "\n",
    "3. Obtain the final sample: The final sample $x_0$ is obtained after completing the reverse process using\n",
    "    significantly fewer steps than DDPM (typically 50-100 vs 1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps):\n",
    "    \"\"\"\n",
    "    Linear beta schedule as used in the DDPM paper\n",
    "    Args:\n",
    "        timesteps: Number of timesteps in the diffusion process\n",
    "    Returns:\n",
    "        Tensor of betas\n",
    "    \"\"\"\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define sampling parameters\n",
    "n_timesteps = 1000  # Original number of timesteps used in training\n",
    "n_sampling_steps = 100  # Number of steps for DDIM (much less than DDPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate necessary variables\n",
    "betas = linear_beta_schedule(n_timesteps).to(device)  # Compute beta schedule and move to device\n",
    "alphas = 1. - betas  # Compute alphas from betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)  # Compute cumulative product of alphas\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)  # Compute square root of cumulative alphas\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)  # Compute square root of (1 - cumulative alphas)\n",
    "\n",
    "# Update timesteps\n",
    "timesteps = torch.linspace(n_timesteps - 1, 0, n_sampling_steps, dtype=torch.long).to(device)  # Create timestep sequence for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Disable gradient calculations for inference\n",
    "def ddim_sample(model, image_size, batch_size=1, channels=3):\n",
    "    \"\"\"\n",
    "    DDIM sampling function\n",
    "    Args:\n",
    "        model: Trained UNet model\n",
    "        image_size: Size of the output image\n",
    "        batch_size: Number of images to generate simultaneously\n",
    "        channels: Number of image channels\n",
    "    Returns:\n",
    "        Generated images\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    # Start from pure noise\n",
    "    x_t = torch.randn(batch_size, channels, image_size, image_size).to(device)\n",
    "\n",
    "    # Progress through timesteps in reverse order\n",
    "    for i in range(len(timesteps) - 1):\n",
    "        t = timesteps[i]\n",
    "        t_prev = timesteps[i + 1]\n",
    "\n",
    "        # Get model prediction (predicted noise)\n",
    "        pred_noise = model(x_t, t)\n",
    "\n",
    "        # Extract alphas for current and previous timesteps\n",
    "        alpha_t = alphas_cumprod[t]\n",
    "        alpha_t_prev = alphas_cumprod[t_prev]\n",
    "\n",
    "        # DDIM deterministic sampling formula\n",
    "        x_0_pred = (x_t - torch.sqrt(1 - alpha_t) * pred_noise) / torch.sqrt(alpha_t)\n",
    "        x_t = torch.sqrt(alpha_t_prev) * x_0_pred + \\\n",
    "              torch.sqrt(1 - alpha_t_prev) * pred_noise\n",
    "\n",
    "    return x_t  # Return final generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"generated_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set generation parameters\n",
    "image_size = 64  # Adjust based on your training image size\n",
    "batch_size = 4   # Number of images to generate at once\n",
    "channels = 3     # RGB images\n",
    "num_images = 100 # Total number of images to generate\n",
    "\n",
    "def save_images(images, start_idx):\n",
    "    \"\"\"\n",
    "    Save tensor images as PNG files\n",
    "    Args:\n",
    "        images: Tensor of normalized images\n",
    "        start_idx: Starting index for file naming\n",
    "    \"\"\"\n",
    "    images = denormalize_images(images)\n",
    "    for i, img in enumerate(images):\n",
    "        # Convert to PIL Image and save\n",
    "        img = Image.fromarray((img * 255).astype('uint8'))\n",
    "        img.save(os.path.join(output_dir, f'generated_image_{start_idx + i:03d}.png'))\n",
    "\n",
    "# Function to convert tensors to displayable images\n",
    "def denormalize_images(images):\n",
    "    \"\"\"\n",
    "    Convert normalized tensor images to displayable format\n",
    "    Args:\n",
    "        images: Tensor of shape (batch_size, channels, height, width)\n",
    "    Returns:\n",
    "        Numpy array of images in range [0, 1]\n",
    "    \"\"\"\n",
    "    images = images.cpu().detach()\n",
    "    images = (images + 1) / 2  # Convert from [-1, 1] to [0, 1]\n",
    "    images = images.clamp(0, 1)\n",
    "    images = images.permute(0, 2, 3, 1)  # Change to (batch, height, width, channels)\n",
    "    return images.numpy()\n",
    "\n",
    "# Generate and save images in batches\n",
    "num_batches = (num_images + batch_size - 1) // batch_size\n",
    "generated_count = 0\n",
    "\n",
    "print(f\"Generating {num_images} images...\")\n",
    "for batch in range(num_batches):\n",
    "    # Adjust batch size for the last batch if needed\n",
    "    current_batch_size = min(batch_size, num_images - generated_count)\n",
    "\n",
    "    # Generate images\n",
    "    with torch.no_grad():\n",
    "        generated_images = ddim_sample(\n",
    "            model=loaded_model,\n",
    "            image_size=image_size,\n",
    "            batch_size=current_batch_size,\n",
    "            channels=channels\n",
    "        )\n",
    "\n",
    "    # Save the generated images\n",
    "    save_images(generated_images, generated_count)\n",
    "    generated_count += current_batch_size\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Progress: {generated_count}/{num_images} images generated\")\n",
    "\n",
    "print(f\"Generation complete. Images saved in '{output_dir}' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FID Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # Resize images to Inception v3 input size\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet stats ie we will also preprocess the data in same way as the original imagenet dataset that was used to train Inception v3\n",
    "])  # Define preprocessing steps for Inception v3 input\n",
    "\n",
    "def prepare_inception_input(images):\n",
    "    return preprocess(images)  # Apply preprocessing to the input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3  # Import inception_v3 model from torchvision\n",
    "\n",
    "def load_inception_model():\n",
    "    \"\"\"\n",
    "    Loads and prepares a pre-trained Inception v3 model for feature extraction\n",
    "\n",
    "    Returns:\n",
    "        model: Modified Inception v3 model with final classification layer removed\n",
    "    \"\"\"\n",
    "    model = inception_v3(pretrained=True, transform_input=False)  # Load pre-trained Inception v3 model without input transformation\n",
    "    model.fc = torch.nn.Identity()  # Removes the last fully connected layer\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model.to(device)  # Move the model to the same device as our GAN\n",
    "\n",
    "inception_model = load_inception_model()  # Load and prepare the modified Inception v3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import glob  # Import glob module for file path operations\n",
    "\n",
    "def extract_inception_features(image_paths):\n",
    "    \"\"\"\n",
    "    Extracts features from images using Inception v3 model\n",
    "\n",
    "    Args:\n",
    "        image_paths: List of paths to image files\n",
    "\n",
    "    Returns:\n",
    "        features: Tensor of extracted features from all images\n",
    "    \"\"\"\n",
    "    all_features = []  # Initialize list to store features from all images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert image to tensor\n",
    "        transforms.Resize((299, 299)),  # Resize to inception input size\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "    ])  # Define transformations for input images\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for img_path in image_paths:  # Process each image\n",
    "            img = Image.open(img_path).convert('RGB')  # Load and convert image to RGB\n",
    "            img_tensor = transform(img).unsqueeze(0).to(device)  # Transform and add batch dimension\n",
    "            features = inception_model(img_tensor)  # Extract features using inception model\n",
    "            all_features.append(features)  # Store features for current image\n",
    "\n",
    "    return torch.cat(all_features, dim=0)  # Concatenate all features into single tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get list of generated image paths\n",
    "generated_image_paths = glob.glob(os.path.join(r\"D:\\Users\\VICTOR\\Desktop\\ADRL\\Assignment 3\\generated_images\", \"*.png\"))  # Get paths of all PNG files in specified directory using os.path.join for robust path handling\n",
    "\n",
    "# Extract features from generated images\n",
    "generated_features = extract_inception_features(generated_image_paths)  # Extract inception features from generated images\n",
    "print(f\"Extracted features from {len(generated_image_paths)} generated images\")  # Print number of processed images\n",
    "print(f\"Feature shape: {generated_features.shape}\")  # Print shape of extracted features\n",
    "\n",
    "# Calculate statistics of generated features\n",
    "generated_mean = torch.mean(generated_features, dim=0)  # Calculate mean across all samples for each feature dimension\n",
    "generated_cov = torch.cov(generated_features.T)  # Calculate covariance matrix of features across samples\n",
    "\n",
    "print(f\"Generated features mean shape: {generated_mean.shape}\")  # Print shape of mean vector\n",
    "print(f\"Generated features covariance shape: {generated_cov.shape}\")  # Print shape of covariance matrix\n",
    "\n",
    "# Save statistics for later comparison\n",
    "torch.save(generated_mean, 'generated_mean.pt')  # Save mean vector to disk for future use\n",
    "torch.save(generated_cov, 'generated_cov.pt')  # Save covariance matrix to disk for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get list of real image paths and verify they exist\n",
    "real_image_paths = glob.glob(r\"D:\\Users\\VICTOR\\Desktop\\ADRL\\Assignment 3\\Butterfly dataset\\*.jpg\")[:100]  # Get paths of 100 JPG files from real dataset\n",
    "if len(real_image_paths) == 0:  # Check if any image paths were found\n",
    "    raise ValueError(\"No image files found in the specified directory\")  # Raise error if no images found\n",
    "\n",
    "# Extract features from real images with error handling\n",
    "try:\n",
    "    real_features = extract_inception_features(real_image_paths)  # Extract inception features from real images\n",
    "    print(f\"Extracted features from {len(real_image_paths)} real images\")  # Print number of processed images\n",
    "    print(f\"Feature shape: {real_features.shape}\")  # Print shape of extracted features\n",
    "\n",
    "    # Calculate statistics of real features\n",
    "    real_mean = torch.mean(real_features, dim=0)  # Calculate mean across all samples for each feature dimension\n",
    "    real_cov = torch.cov(real_features.T)  # Calculate covariance matrix of features across samples\n",
    "\n",
    "    print(f\"Real features mean shape: {real_mean.shape}\")  # Print shape of mean vector\n",
    "    print(f\"Real features covariance shape: {real_cov.shape}\")  # Print shape of covariance matrix\n",
    "\n",
    "    # Save statistics for later comparison\n",
    "    torch.save(real_mean, 'real_mean.pt')  # Save mean vector to disk for future use\n",
    "    torch.save(real_cov, 'real_cov.pt')  # Save covariance matrix to disk for future use\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error processing images: {str(e)}\")  # Print error message if feature extraction fails\n",
    "    print(\"Please verify that all images are valid and accessible\")  # Provide troubleshooting hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_frechet_inception_distance(real_mean, real_cov, generated_mean, generated_cov):\n",
    "    \"\"\"\n",
    "    Calculate the Fréchet Inception Distance (FID) between real and generated image features.\n",
    "\n",
    "    Args:\n",
    "    real_mean (torch.Tensor): Mean of real image features.\n",
    "    real_cov (torch.Tensor): Covariance matrix of real image features.\n",
    "    generated_mean (torch.Tensor): Mean of generated image features.\n",
    "    generated_cov (torch.Tensor): Covariance matrix of generated image features.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated FID score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to numpy for scipy operations\n",
    "    real_mean_np = real_mean.cpu().numpy()  # Convert real mean to numpy array\n",
    "    real_cov_np = real_cov.cpu().numpy()  # Convert real covariance to numpy array\n",
    "    generated_mean_np = generated_mean.cpu().numpy()  # Convert generated mean to numpy array\n",
    "    generated_cov_np = generated_cov.cpu().numpy()  # Convert generated covariance to numpy array\n",
    "\n",
    "    # Calculate squared L2 norm between means\n",
    "    mean_diff = np.sum((real_mean_np - generated_mean_np) ** 2)  # Compute squared difference between means\n",
    "\n",
    "    # Calculate sqrt of product of covariances\n",
    "    covmean = scipy.linalg.sqrtm(real_cov_np.dot(generated_cov_np))  # Compute matrix square root\n",
    "\n",
    "    # Check and correct imaginary parts if necessary\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real  # Take only the real part if result is complex\n",
    "\n",
    "    # Calculate trace term\n",
    "    trace_term = np.trace(real_cov_np + generated_cov_np - 2 * covmean)  # Compute trace of the difference\n",
    "\n",
    "    # Compute FID\n",
    "    fid = mean_diff + trace_term  # Sum up mean difference and trace term\n",
    "\n",
    "    return fid  # Return FID as a Python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "# Move tensors to CPU and convert to numpy arrays\n",
    "generated_mean_cpu = generated_mean.cpu()  # Move generated mean tensor to CPU\n",
    "generated_cov_cpu = generated_cov.cpu()  # Move generated covariance tensor to CPU\n",
    "real_mean_cpu = real_mean.cpu()  # Move real mean tensor to CPU\n",
    "real_cov_cpu = real_cov.cpu()  # Move real covariance tensor to CPU\n",
    "\n",
    "# Calculate FID score using CPU tensors\n",
    "fid_score = calculate_frechet_inception_distance(real_mean_cpu, real_cov_cpu, generated_mean_cpu, generated_cov_cpu)  # Calculate FID using CPU tensors\n",
    "print(f\"Fréchet Inception Distance: {fid_score:.4f}\")  # Print calculated FID score with 4 decimal places"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
